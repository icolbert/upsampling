{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import upsampling.layers as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating that custom layers are equivalent to PyTorch implementations\n",
    "\n",
    "- Conv2d\n",
    "- PixelShuffle\n",
    "- ConvTranspose2d (Deconvolution)\n",
    "\n",
    "These layers are written for code clarity, not for optimization. Because each layer is written as nested for-loops, its best to use small values for feature map dimensions, upscaling factors, and kernel sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2            MSE = 0.000\n",
      "PixelShuffle     MSE = 0.000\n",
      "Sub-Pixel Conv   MSE = 0.000\n",
      "Deconvolution    MSE = 0.000\n"
     ]
    }
   ],
   "source": [
    "H = W = 4 # height/width - note using square feature maps\n",
    "C = 3     # number of channels - note that the squeeze() operation below means this check only works for C > 1\n",
    "K = 3     # kernel_size\n",
    "N = 1     # batch_size\n",
    "r = 2     # upscaling factor\n",
    "\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "# Testing the Conv2d Operator\n",
    "x = torch.randn(N, C, H, W)\n",
    "\n",
    "m = nn.Conv2d(in_channels=C, out_channels=C, kernel_size=K, bias=False, padding=1)\n",
    "n = L.Conv2d(in_channels=C, out_channels=C, kernel_size=K, padding=1)\n",
    "n.weight = m.weight\n",
    "\n",
    "print(f\"Conv2            MSE = {(m(x).squeeze() - n(x.squeeze())).pow(2).mean().item():.3f}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "# Testing the PixelShuffle Operator\n",
    "x = torch.zeros(N, C*(r**2), H, W)\n",
    "\n",
    "for c in range(C*(r**2)):\n",
    "    x[:,c,:,:] = c\n",
    "\n",
    "m = nn.PixelShuffle(r)\n",
    "n = L.PixelShuffle(r)\n",
    "\n",
    "print(f\"PixelShuffle     MSE = {(m(x).squeeze() - n(x.squeeze())).pow(2).mean().item():.3f}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "# Testing the Sub-Pixel Convolution Operator\n",
    "x = torch.randn(N, C, H, W)\n",
    "m_layer1 = nn.Conv2d(in_channels=C, out_channels=C*(r**2), kernel_size=K, padding=1, bias=False)\n",
    "m_layer2 = nn.PixelShuffle(r)\n",
    "p = m_layer2(m_layer1(x)).squeeze()\n",
    "\n",
    "n_layer1 = L.Conv2d(in_channels=C, out_channels=C*(r**2), kernel_size=K, padding=1)\n",
    "n_layer2 = L.PixelShuffle(r)\n",
    "n_layer1.weight = m_layer1.weight\n",
    "q = n_layer2(n_layer1(x.squeeze()))\n",
    "\n",
    "print(f\"Sub-Pixel Conv   MSE = {(p - q).pow(2).mean().item():.3f}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "# Testing the Deconvolution Operator\n",
    "x = torch.randn(N, C, H, W)\n",
    "m = nn.ConvTranspose2d(in_channels=C, out_channels=C, kernel_size=K*r, stride=r, padding=r, bias=False)\n",
    "n = L.Deconvolution(in_channels=C, out_channels=C, kernel_size=K*r, stride=r, padding=r)\n",
    "n.weight = m.weight = m.weight\n",
    "\n",
    "print(f\"Deconvolution    MSE = {(m(x).squeeze() - n(x.squeeze())).pow(2).mean().item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing that a sub-pixel convolution is equivalent to a deconvolution using the WeightShuffle operator\n",
    "\n",
    "[Shi *et al.* (2016)- Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158)\n",
    "\n",
    "[Shi *et al.* (2016) - Is the deconvolution layer the same as a convolutional layer?](https://arxiv.org/abs/1609.07009)\n",
    "\n",
    "Given that the convolution kernel size is 3, the sub-pixel convolution can be transformed into a deconvolution. This allows a hardware designer to separate training from inference - software from hardware - when accelerating upsampling solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-pixel Convolution versus Deconvolution    MSE = 0.000\n"
     ]
    }
   ],
   "source": [
    "H = W = 2\n",
    "K = 3\n",
    "r = 2\n",
    "\n",
    "x = torch.randn(N, C, H, W)\n",
    "\n",
    "# Sub-pixel convolution operation\n",
    "convolution   = L.Conv2d(in_channels=C, out_channels=C*(r**2), kernel_size=K, stride=1, padding=1)\n",
    "pixel_shuff   = L.PixelShuffle(r)\n",
    "\n",
    "# Deconvolution operation\n",
    "deconvolution = L.Deconvolution(in_channels=C, out_channels=C, kernel_size=K*r, stride=r, padding=r)\n",
    "weight_shuff  = L.WeightShuffle(r)\n",
    "\n",
    "# Shuffle convolution weights to be equivalent to the deconvolution\n",
    "deconvolution.weight = weight_shuff(convolution.weight)\n",
    "\n",
    "# Run and compare\n",
    "y_conv   = pixel_shuff(convolution(x.squeeze()))\n",
    "y_deconv = deconvolution(x.squeeze())\n",
    "\n",
    "print(f\"Sub-pixel Convolution versus Deconvolution    MSE = {(y_conv - y_deconv).pow(2).mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing that the deconvolution operators give the identical results\n",
    "\n",
    "[Zhang *et al.* (2017) - A Design Methodology for Efficient Implementation of Deconvolutional Neural Networks on an FPGA](https://arxiv.org/abs/1705.02583)\n",
    "\n",
    "[Colbert *et al* (2021) - A Competitive Edge: Can FPGAs Beat GPUs at DCNN Inference Acceleration in Resource-Limited Edge Computing Applications?](https://arxiv.org/abs/2102.00294)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deconvolution    MSE = 0.000\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------- #\n",
    "# Testing the Standard Deconvolution operator against the Reverse Deconvolution operator\n",
    "x = torch.randn(N, C, H, W)\n",
    "m = L.Deconvolution(\n",
    "    in_channels=C, out_channels=C, kernel_size=K*r, stride=r, padding=r, algorithm=L.DeconvolutionAlgorithms.STDD\n",
    ")\n",
    "n = L.Deconvolution(\n",
    "    in_channels=C, out_channels=C, kernel_size=K*r, stride=r, padding=r, algorithm=L.DeconvolutionAlgorithms.REVD\n",
    ")\n",
    "n.weight = m.weight = m.weight\n",
    "\n",
    "print(f\"Deconvolution    MSE = {(n(x.squeeze()) - m(x.squeeze())).pow(2).mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deconvolution    MSE = 0.000\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------- #\n",
    "# Testing the Standard Deconvolution operator against the Reverse Deconvolution-2 operator\n",
    "x = torch.randn(N, C, H, W)\n",
    "m = L.Deconvolution(\n",
    "    in_channels=C, out_channels=C, kernel_size=K*r, stride=r, padding=r, algorithm=L.DeconvolutionAlgorithms.STDD\n",
    ")\n",
    "n = L.Deconvolution(\n",
    "    in_channels=C, out_channels=C, kernel_size=K*r, stride=r, padding=r, algorithm=L.DeconvolutionAlgorithms.REVD2\n",
    ")\n",
    "n.weight = m.weight = m.weight\n",
    "\n",
    "print(f\"Deconvolution    MSE = {(n(x.squeeze()) - m(x.squeeze())).pow(2).mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.000\n"
     ]
    }
   ],
   "source": [
    "H = W = 3\n",
    "K = 3\n",
    "r = 2\n",
    "C = 1\n",
    "\n",
    "x = torch.randn(N, C, H, W)\n",
    "m = nn.Upsample(scale_factor=r, mode='nearest')\n",
    "n = nn.Conv2d(in_channels=C, out_channels=C, kernel_size=K, padding=1, stride=1, bias=False)\n",
    "\n",
    "z = torch.zeros(C, C, 2 + r, 2 + r)\n",
    "for i in range(0, r):\n",
    "    for j in range(0, r):\n",
    "        z[:,:,i:i+K,j:j+K] += torch.rot90(n.weight.data, 2, [2,3])\n",
    "\n",
    "n2 = nn.ConvTranspose2d(in_channels=C, out_channels=C, kernel_size=r+2, stride=r, padding=1, bias=False)\n",
    "n2.weight.data = z\n",
    "\n",
    "print(f\"MSE = {(n(m(x)) - n2(x)).pow(2).mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = W = 2\n",
    "K = 3\n",
    "r = 3\n",
    "C = 1\n",
    "\n",
    "x = torch.zeros(N, C, H, W)\n",
    "x[0,0,0,1] = 1\n",
    "n = nn.ConvTranspose2d(in_channels=C, out_channels=C, kernel_size=r+2, stride=r, padding=1, bias=False)\n",
    "n.weight.data /= n.weight.data\n",
    "n(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rot90(n.weight.data, 2, [2,3]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
